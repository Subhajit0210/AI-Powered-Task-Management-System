{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6860775",
   "metadata": {},
   "source": [
    "# AI-Powered Task Management System\n",
    "\n",
    "This notebook implements an AI-powered task management system using NLP and ML techniques to analyze and process task data from Jira."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1428c21",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5b4117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "from IPython.display import display\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a6d8ce",
   "metadata": {},
   "source": [
    "## 2. Load and Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714f4ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('jira_dataset.csv')\n",
    "    print(f\"Successfully loaded data with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File 'jira_dataset.csv' not found\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {str(e)}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6106398",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751ec706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where clean_summary is null\n",
    "print(f\"Number of rows before dropping nulls: {df.shape[0]}\")\n",
    "df = df.dropna(subset=['clean_summary'])\n",
    "print(f\"Number of rows after dropping nulls: {df.shape[0]}\")\n",
    "\n",
    "# Standardize deadline entries\n",
    "def standardize_deadline(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    \n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Common replacements\n",
    "    replacements = {\n",
    "        'tow': 'two',\n",
    "        'for days': 'four days',\n",
    "        'tree': 'three',\n",
    "        'won': 'one',\n",
    "        'to': 'two',\n",
    "        'free': 'three'\n",
    "    }\n",
    "    \n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply standardization to deadline column\n",
    "if 'deadline' in df.columns:\n",
    "    df['deadline'] = df['deadline'].apply(standardize_deadline)\n",
    "    print(\"\\nSample of standardized deadlines:\")\n",
    "    display(df['deadline'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edf6fc7",
   "metadata": {},
   "source": [
    "## 4. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe4cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove punctuation and numbers\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation + string.digits))\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Join tokens back into string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to clean_summary column\n",
    "if 'clean_summary' in df.columns:\n",
    "    df['processed_summary'] = df['clean_summary'].apply(preprocess_text)\n",
    "    \n",
    "    # Display sample of original and processed summaries\n",
    "    print(\"Sample of original and processed summaries:\")\n",
    "    display(pd.DataFrame({\n",
    "        'Original': df['clean_summary'].head(),\n",
    "        'Processed': df['processed_summary'].head()\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3799cb18",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb94bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot issue type distribution\n",
    "if 'issue_type' in df.columns:\n",
    "    sns.countplot(data=df, y='issue_type', ax=axes[0,0])\n",
    "    axes[0,0].set_title('Distribution of Issue Types')\n",
    "\n",
    "# Plot priority distribution\n",
    "if 'priority' in df.columns:\n",
    "    sns.countplot(data=df, x='priority', ax=axes[0,1])\n",
    "    axes[0,1].set_title('Distribution of Priorities')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot task assignee distribution (top 10)\n",
    "if 'task_assignee' in df.columns:\n",
    "    top_assignees = df['task_assignee'].value_counts().head(10)\n",
    "    sns.barplot(x=top_assignees.values, y=top_assignees.index, ax=axes[1,0])\n",
    "    axes[1,0].set_title('Top 10 Task Assignees')\n",
    "\n",
    "# Plot summary statistics\n",
    "if 'processed_summary' in df.columns:\n",
    "    word_counts = df['processed_summary'].str.split().str.len()\n",
    "    sns.histplot(data=word_counts, bins=30, ax=axes[1,1])\n",
    "    axes[1,1].set_title('Distribution of Word Counts in Processed Summaries')\n",
    "    axes[1,1].set_xlabel('Number of Words')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "display(df.describe())\n",
    "\n",
    "# Display value counts for categorical columns\n",
    "print(\"\\nValue counts for categorical columns:\")\n",
    "for col in ['issue_type', 'priority', 'task_assignee']:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n{col}:\")\n",
    "        display(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2191e1a9",
   "metadata": {},
   "source": [
    "## 6. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ac637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for model training\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "\n",
    "# Prepare the data\n",
    "X = df['processed_summary']\n",
    "y = df['issue_type']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(\"Data split and vectorization completed:\")\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55add22",
   "metadata": {},
   "source": [
    "### Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a19c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1 Score: {f1:.3f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Train and evaluate Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "nb_metrics = evaluate_model(nb_model, X_test_tfidf, y_test, 'Naive Bayes')\n",
    "\n",
    "# Train and evaluate LinearSVC\n",
    "svc_model = LinearSVC(random_state=42)\n",
    "svc_model.fit(X_train_tfidf, y_train)\n",
    "svc_metrics = evaluate_model(svc_model, X_test_tfidf, y_test, 'LinearSVC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d3c2ef",
   "metadata": {},
   "source": [
    "### Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d98b812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models and save the best one\n",
    "nb_f1 = nb_metrics[3]\n",
    "svc_f1 = svc_metrics[3]\n",
    "\n",
    "if nb_f1 > svc_f1:\n",
    "    best_model = nb_model\n",
    "    best_model_name = 'Naive Bayes'\n",
    "else:\n",
    "    best_model = svc_model\n",
    "    best_model_name = 'LinearSVC'\n",
    "\n",
    "# Save the best model and vectorizer\n",
    "joblib.dump(best_model, 'task_classifier.pkl')\n",
    "joblib.dump(tfidf, 'tfidf_vectorizer.pkl')\n",
    "\n",
    "print(f\"\\nBest model ({best_model_name}) saved as 'task_classifier.pkl'\")\n",
    "print(\"TF-IDF vectorizer saved as 'tfidf_vectorizer.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8533e419",
   "metadata": {},
   "source": [
    "## 7. Priority Prediction and Workload Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4383b74b",
   "metadata": {},
   "source": [
    "### Priority Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dcdaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for priority prediction\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# Prepare the data for priority prediction\n",
    "X_priority = df['processed_summary']\n",
    "y_priority = df['priority']\n",
    "\n",
    "# Split the data\n",
    "X_train_priority, X_test_priority, y_train_priority, y_test_priority = train_test_split(\n",
    "    X_priority, y_priority, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create and fit TF-IDF vectorizer for priority prediction\n",
    "tfidf_priority = TfidfVectorizer(max_features=5000)\n",
    "X_train_priority_tfidf = tfidf_priority.fit_transform(X_train_priority)\n",
    "X_test_priority_tfidf = tfidf_priority.transform(X_test_priority)\n",
    "\n",
    "print(\"Data prepared for priority prediction:\")\n",
    "print(f\"Training set size: {X_train_priority.shape[0]}\")\n",
    "print(f\"Test set size: {X_test_priority.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92208132",
   "metadata": {},
   "source": [
    "### Model Training with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2478a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for Random Forest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Create and train Random Forest with GridSearchCV\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_grid = GridSearchCV(rf_model, rf_param_grid, cv=5, scoring='f1_weighted', n_jobs=-1)\n",
    "rf_grid.fit(X_train_priority_tfidf, y_train_priority)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best Random Forest parameters:\")\n",
    "print(rf_grid.best_params_)\n",
    "\n",
    "# Get best model\n",
    "best_rf_model = rf_grid.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred_priority = best_rf_model.predict(X_test_priority_tfidf)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_priority, y_pred_priority))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test_priority, y_pred_priority)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Priority Prediction')\n",
    "plt.xlabel('Predicted Priority')\n",
    "plt.ylabel('Actual Priority')\n",
    "plt.show()\n",
    "\n",
    "# Save the model and vectorizer\n",
    "joblib.dump(best_rf_model, 'priority_predictor.pkl')\n",
    "joblib.dump(tfidf_priority, 'priority_tfidf_vectorizer.pkl')\n",
    "print(\"\\nModel and vectorizer saved as 'priority_predictor.pkl' and 'priority_tfidf_vectorizer.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d5d5e1",
   "metadata": {},
   "source": [
    "### Workload Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71365d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate tasks per assignee\n",
    "tasks_per_assignee = df['task_assignee'].value_counts()\n",
    "\n",
    "# Create figure for workload visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=tasks_per_assignee.index, y=tasks_per_assignee.values)\n",
    "plt.title('Number of Tasks per Assignee')\n",
    "plt.xlabel('Assignee')\n",
    "plt.ylabel('Number of Tasks')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify assignee with least workload\n",
    "min_workload_assignee = tasks_per_assignee.idxmin()\n",
    "min_workload = tasks_per_assignee.min()\n",
    "\n",
    "print(f\"\\nAssignee with least workload: {min_workload_assignee}\")\n",
    "print(f\"Number of tasks: {min_workload}\")\n",
    "\n",
    "# Calculate workload statistics\n",
    "print(\"\\nWorkload Statistics:\")\n",
    "print(f\"Average tasks per assignee: {tasks_per_assignee.mean():.2f}\")\n",
    "print(f\"Median tasks per assignee: {tasks_per_assignee.median():.2f}\")\n",
    "print(f\"Maximum tasks per assignee: {tasks_per_assignee.max()}\")\n",
    "\n",
    "# Create workload distribution plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=tasks_per_assignee.values, bins=20)\n",
    "plt.title('Distribution of Workload per Assignee')\n",
    "plt.xlabel('Number of Tasks')\n",
    "plt.ylabel('Number of Assignees')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f127d67b",
   "metadata": {},
   "source": [
    "### Workload Balance Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfc2a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate workload imbalance\n",
    "workload_imbalance = tasks_per_assignee.max() - tasks_per_assignee.min()\n",
    "print(f\"Workload imbalance (max - min tasks): {workload_imbalance}\")\n",
    "\n",
    "# Identify overloaded assignees (more than 1.5 times the median)\n",
    "median_tasks = tasks_per_assignee.median()\n",
    "overloaded_assignees = tasks_per_assignee[tasks_per_assignee > (1.5 * median_tasks)]\n",
    "print(\"\\nOverloaded assignees (more than 1.5 times median workload):\")\n",
    "for assignee, tasks in overloaded_assignees.items():\n",
    "    print(f\"{assignee}: {tasks} tasks\")\n",
    "\n",
    "# Recommend task redistribution\n",
    "print(\"\\nTask Redistribution Recommendations:\")\n",
    "print(f\"1. Consider reassigning tasks from overloaded assignees to {min_workload_assignee}\")\n",
    "print(\"2. Target workload per assignee should be around the median:\", f\"{median_tasks:.1f} tasks\")\n",
    "print(\"3. Current workload imbalance:\", f\"{workload_imbalance} tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c99ecad",
   "metadata": {},
   "source": [
    "## 8. Deploying the Model with Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fec82d3",
   "metadata": {},
   "source": [
    "### Running the Streamlit App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c4aaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install streamlit pandas matplotlib seaborn scikit-learn joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d81bcb",
   "metadata": {},
   "source": [
    "### Streamlit App Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14e8c27",
   "metadata": {},
   "source": [
    "To run the Streamlit app:\n",
    "\n",
    "1. Ensure all required files are present:\n",
    "   - streamlit_app.py\n",
    "   - task_classifier.pkl\n",
    "   - priority_predictor.pkl\n",
    "   - tfidf_vectorizer.pkl\n",
    "   - priority_tfidf_vectorizer.pkl\n",
    "   - jira_dataset.csv\n",
    "\n",
    "2. Open a terminal and navigate to the project directory\n",
    "\n",
    "3. Run the app with:\n",
    "   ```bash\n",
    "   streamlit run streamlit_app.py\n",
    "   ```\n",
    "\n",
    "4. The app will open in your default web browser\n",
    "\n",
    "The Streamlit app provides:\n",
    "- A text input area for task descriptions\n",
    "- Automatic prediction of issue type and priority\n",
    "- Real-time workload visualization\n",
    "- Workload statistics and recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74197da",
   "metadata": {},
   "source": [
    "### App Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a749a9c7",
   "metadata": {},
   "source": [
    "The Streamlit app includes:\n",
    "\n",
    "1. **Task Prediction**\n",
    "   - Enter task description\n",
    "   - Get predicted issue type and priority\n",
    "   - Real-time predictions\n",
    "\n",
    "2. **Workload Visualization**\n",
    "   - Bar chart of tasks per assignee\n",
    "   - Workload statistics\n",
    "   - Team workload distribution\n",
    "\n",
    "3. **User Interface**\n",
    "   - Clean, modern design\n",
    "   - Responsive layout\n",
    "   - Clear instructions\n",
    "   - Error handling"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
